# Model
model:
  encoder:
    name: regnety_032
    params:
      timm_feat_len: 1512 # 032->1512, 040->1088
      enc_feat_len: 256
      dropout_prob: 0.3
      in_channels: 3
  decoder:
    name: GRUDecoder
    params:
      hidden_size: 128
      num_layers: 1
      fc_size: 256
      dropout_prob: 0.3

# Training loop settings
training:
  num_steps: &numsteps 150000
  log_steps: 50
  val_interval: 1000
  num_log_imgs: 8
  checkpoint_dir: /home/martin/datasets/comma2k19/checkpoints
  amp_backend: native
  batch_size: 10
  num_workers: 8

# Dataset settings
dataset:
  dataset_file: full_trainval_set.json
  future_steps: 30 # 1.5 seconds @ 20 fps
  past_steps: 4
  channel_concat: false
  predict_speed: false

# Augmentations
# For img_augs: Use exact name in torchvision as the key and its kwargs as the value
# For full_augs: Use exact name in data/transforms.py as the key and its kwargs as the value
# Normalize should be last in the img_augs
train_augs:
  img_augs:
    GaussianBlur:
      kernel_size: [3, 5]
      sigma: [0.01, 2]
    ColorJitter:
      brightness: 0.7
      contrast: 0.6
      saturation: 0.7
      hue: 0.2
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  full_augs:
    RandomHorizontalFlip:
      prob: 0.5

val_augs:
  Normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# Provide exact name from torch.optim library and kwargs
scheduler:
  name: OneCycleLR
  kwargs:
    max_lr: 0.01
    total_steps: *numsteps

# Provide exact name from torch.optim.lr_scheduler and kwargs
optimizer:
  name: AdamW
  kwargs:
    lr: 1.0e-3
    betas: [0.9, 0.999]
    eps: 1.0e-7
    weight_decay: 8.0e-4
    