# Model
model:
  encoder:
    name: regnety_032
    params:
      timm_feat_len: 1512 # 032->1512, 040->1088
      enc_feat_len: 256
      dropout_prob: 0.4
      in_channels: 6
  decoder:
    name: FCDecoder
    params:
      fc_size: 256
      dropout_prob: 0.4
      bias_init_path: /home/martin/projects/end-to-end-driving/data/dataset_lists/mean_path.txt

# Training loop settings
training:
  num_steps: &numsteps 100000
  log_steps: 50
  val_interval: 1000
  num_log_imgs: 8
  checkpoint_dir: /home/martin/datasets/comma2k19/checkpoints
  amp_backend: native
  seq_model_decoder: false
  batch_size: 32
  num_workers: 8

# Dataset settings
dataset:
  dataset_file: full_trainval_set_curv.json
  future_steps: 30 # 1.5 seconds @ 20 fps
  past_steps: 1
  channel_concat: true
  # Weighing factor in range [0, 1], set to 0 for no weighing
  # Bin weights are equal to 1 / count^weighting_factor
  weighing_factor: 0.25
  num_bins: 100

# Augmentations
# For img_augs: Use exact name in torchvision as the key and its kwargs as the value
# For full_augs: Use exact name in data/transforms.py as the key and its kwargs as the value
# Normalize should be last in the img_augs
train_augs:
  img_augs:
    GaussianBlur:
      kernel_size: [3, 5]
      sigma: [0.01, 2]
    ColorJitter:
      brightness: 0.8
      contrast: 0.7
      saturation: 0.8
      hue: 0.3
    Normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  full_augs:
    RandomHorizontalFlip:
      prob: 0.5

val_augs:
  Normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]

# Provide exact name from torch.optim library and kwargs
scheduler:
  name: OneCycleLR
  kwargs:
    max_lr: 0.01
    total_steps: *numsteps

# Provide exact name from torch.optim.lr_scheduler and kwargs
optimizer:
  name: AdamW
  kwargs:
    lr: 1.0e-3
    betas: [0.9, 0.999]
    eps: 1.0e-7
    weight_decay: 8.0e-4
    